# Raft (2014)

## Abstract
- Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos.
- Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered.
- Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety

## 1. Introduction
- Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.
- Paxos has dominated the discussion of consensus algorithms over the last decade: most implementations of consensus are based on Paxos of influenced by it. But it is quite difficult to understand
- Raft is similar to many ways to existing consensus algorithms, but it has several novel features:
  - **Strong leader**: Raft uses a stronger form of leadership then other consensus algorithms. For example, log entries only flow from the leader to other servers. This simplifies the management of the replicated log and makes Raft easier to understand.
  - **Leader election**: Raft uses randomized timers to elect leaders. This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly
  - **Membership changes**: Raft's mechanism for changing the set of servers in the cluster uses a new join consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes

## 2. Replicated state machines
- Consensus algorithms typically arise in the context of replicated state machines. In this approach, state machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems
- Replicated state machines are typically implemented using a replicated log. Each server stores a log containing a series of commands, which its state machine executes in order. Each log contains the same commands in the same order, so each state machine processes the same sequence of commands. Since the state machines are deterministic, each computes the same state and the same sequence of outputs
![alt text](/resource/image/Raft(2014)-figure-1.png)
- Keeping the replicated log consistent is the job of the consensus algorithm. The consensus module on a server receives commands from clients and adds them to its log. It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are properly replicated, each server's state machine processes them in log order, and the outputs are returned to clients.
- Consensus algorithms for practical systems typically have the following properties:
  - They ensure safety under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering
  - They are fully functional as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster.
  - They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems.
  - In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.

## 3. What's wrong with Paxos?
- Complexity: Paxos is notoriously difficult to understand.
- Practical Implementation: Paxos does not provide a good foundation for building practical systems.

## 4. Designing for understandability
- Problem Decomposition: They divided the consensus problem into separate components that could be solved and understood independently.
- State Space Reduction: They simplified the state space by reducing the number of states to consider, making the system more coherent and eliminating nondeterminism where possible.

## 5. The Raft consensus algorithm
- Raft is an algorithm for managing a replicated log
- Raft implements consensus by first electing a distinguished *leader*, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. Having a leader simplifies the management of the replicated log.
- Given the leader approach. Raft decomposes the consensus problem into three relatively independent subproblems, which are discussed in the subsections that follow:
  - **Leader election**: a new leader must be chosen when an existing leader fails
  - **Log replication**: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own
  - **Safety**: the key safety property for Raft is the State Machine Safety Property in Figure 3: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index

## 5.1. Raft basics
- A Raft cluster contains several server; five is a typical number, which allows the system tolerate two failures.

![alt text](/resource/image/Raft(2014)-figure-4-5.png)
- At any given time each server is in one of three states: *leader, follower, or candidate*. There is exactly one leader and all of the other servers are followers:
  - Followers are passive
  - The leader handles all client requests
  - The third statem candidate, is used to elect a new leader
- Raft divides time into terms of arbitrary length. Terms are numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term
- Raft ensures that there is at most one leader in a given term.
- Raft servers communicate using remote procedure calls (RPCs), and the basic consensus algorithm requires only two types of RPCs.
  - RequestVote RPCs are initiated by candidates during elections
  - AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat

## 5.2. Leader election
- Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers. A server remains in follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats(AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the *election timeout*, then it assumes there is no viable leader and begins an election to choose a new leader.
- To begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: 
  - (a) it wins the election
  - (b) another server establishes itself as leader
  - (c) a priod of time goes by with no winner
- A candidate wins an elections if it receives votes from a majority of the servers in the full cluster for the same term
- Each server will vote for at most one candidate in a given term, on a first-come-first-served basis
- The majority rule ensures that at most one candidate can win the election for a particuler term
- Once a candidate wins a election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.
- If a candidate receives an AppendEntries RPC from another server claiming to be leader, it returns to follower state if the leader's term is at least as large as its current term.
- Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly

## 5.3. Log replication
- Once a leader has been elected, it begin servicing client request. 
- Each client request contains a command to be executed by replicated state machines.
- The leader appends the command to its log as new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry.
- Whe the entry has been safely replicated, the leader applies the entry to its state machine and returns the result of that execution to the client. If followers crash or run slowly, or if network packets are lost, the leader retries AppendEntries RPCs indefinitely until all followers eventually store all log entries
- Each log entry stores a state machine command along with the term number when the entry was received by the leader.
- The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3

![alt text](/resource/image/Raft(2014)-figure-3.png)

- Logs are organized as shown in Figure 6.

![alt text](/resource/image/Raft(2014)-figure-6.png)

- Each log entry also has an integer index identifying its position in the log.
- The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed.
- A log entry is committed once the leader that created the entry has replicated it on a majority of the servers. This also commits all preceding entries in the leader's log, including entries created by previous leader.
- The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs so that the other servers eventulally find out.
- Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order)
- Raft maintains the following properties, which together constitute the Log Matching Property:
  - If two entries in different logs have the same index and term, then they store the same command.
  - If two entries in different logs have the same index and term, then the logs are identical in all preceding entries
- The first property follows from the fact that a leader creates at most one entry with a given log index in a given term, and log entries never change their position in the log
- The second property is guaranteed by a simple consistency check performed by AppendEntries
- When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, the it refuses the new entries.
- The consistency check acts as an induction step: the initial empty state of the logs satisfies the Log Matching Property, and the consistency check preserves the Log Matching Property whenever logs are extended. As a result, whenever AppendEntries returns successfully, the leader knows that the follower's log is identical to its own log up through the new entries
- In Raft, the leader handles inconsistencies by forcing the follower's logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader's log.
- Handling Inconsistencies: If the logs are inconsistent, the leader decrements the nextIndex and retries the AppendEntries RPC until the logs match. Once they match, the follower's log is updated to be consistent with the leader's log.

## 5.4. Safety
- This section completes the Raft algorithm by adding a restriction on which servers may be elected leader.
- The restriction ensures that the leader for any given term contains all of the entries committed in previous terms

### 5.4.1. Election restriction
- Raft uses a simpler approach where it guarantees that all the committed entries from previous terms are present on each new leader from the moment of its election, without the need to transfer those entries to the leader. This means that log entries only flow in one direction, from leaders to followers, and leaders never overwrite existing entries in their logs.
- Raft uses the voting process to prevent a candidate from winning an elction unless its log contains all committed entries. A candidate must contact a majority of the cluster in order to elected, which means that every committed entry must be present in at least one of those servers. 
- A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers.
- If the candidate's log is a least as up-to-date as any other log in that majority, then it will hold all the committed entries. The RequestVote RPC implements this restriction: the RPC includes information about the candidate's log, and the voter denies its vote if its own log is more up-to-date than that of the candidate
- Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs

### 5.4.2. Committing entries from previous terms
- Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader's current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property
- Raft incurs this extra complexity in the commitment rules because log entries retain their original term numbers when a leader replicates entries from previous terms.
- Raft's approach makes it easier to reason about log entries, since they maintain the same term number over time and across logs. In addition, new leaders in Raft send fewer log entries from previous terms than in other algorithms

### 5.4.3. Safety Argument:
- The Leader Completeness Property ensures that if a leader commits a log entry, all future leaders will also contain that entry. This is crucial for maintaining consistency across the replicated state machines.
- The argument is based on the assumption that if a leader commits a log entry, it must be stored on a majority of the servers. Therefore, any future leader must have this entry in its log.
- The proof involves showing that if a leader commits an entry, then all future leaders will have that entry in their logs due to the Log Matching Property and the election process.
 
## 5.5. Follower and candidate crashes
- If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. 
- Raft handles these failures by retrying indefinitely; if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs are idempotent, so this causes no harm

## 5.6. Timing and availability
- Raft ensures that safety does not depend on timing, meaning the system will not produce incorrect results due to unexpected timing of events. However, availability, or the system's ability to respond to clients in a timely manner, does depend on timing. For Raft to maintain a steady leader and make progress, the following timing requirement must be satisfied:
  - broadcastTime ≪ electionTimeout ≪ MTBF
- broadcastTime: The average time it takes for a server to send RPCs in parallel to every server in the cluster and receive their responses.
- electionTimeout: The election timeout described in Section 5.2.
- MTBF: The average time between failures for a single server.
- The broadcast time should be much shorter than the election timeout to ensure leaders can reliably send heartbeat messages and prevent followers from starting elections. The election timeout should be much shorter than the MTBF to ensure the system makes steady progress. If the leader crashes, the system will be unavailable for roughly the election timeout duration, which should represent only a small fraction of the overall time.
- Typical broadcast times range from 0.5ms to 20ms, depending on storage technology, leading to election timeouts between 10ms and 500ms. Typical server MTBFs are several months or more, easily satisfying the timing requirement.